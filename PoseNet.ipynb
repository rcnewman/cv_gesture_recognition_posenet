{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PoseNet",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcnewman/cv_gesture_recognition_posenet/blob/master/PoseNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4afsWr4n0nJ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3dbdf3df-5586-4522-ad3c-a4f93892938b"
      },
      "source": [
        "# PoseNet-python Notebook Source: https://colab.research.google.com/drive/1Ha7-lt-WwcCx961cuatjOzQ-7GLY91aQ#scrollTo=k_qlzhSXN3D7\n",
        "# install\n",
        "!git clone https://github.com/rcnewman/posenet-python.git #fork from https://www.github.com/rwightman/posenet-python \n",
        "%cd posenet-python\n",
        "!pip3 install tensorflow-gpu==1.15\n",
        "!pip3 install scipy\n",
        "!pip3 install pyyaml\n",
        "!pip3 install opencv-python\n",
        "!pip3 install ipykernel\n",
        "!pip3 install msgpack-numpy\n",
        "!pip3 install ffmpeg-python"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'posenet-python'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 143 (delta 11), reused 17 (delta 7), pack-reused 120\u001b[K\n",
            "Receiving objects: 100% (143/143), 42.86 KiB | 8.57 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n",
            "/content/posenet-python\n",
            "Collecting tensorflow-gpu==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.32.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 53.8MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 56.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.35.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.18.5)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.12.4)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (50.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=31c7db0255b8721f7004929fb848d74ee4bab0c23f563e4432c215a629348eeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, keras-applications, gast, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.18.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (4.10.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel) (4.3.3)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel) (5.3.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel) (50.3.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel) (2.6.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel) (4.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel) (2.8.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel) (19.0.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel) (0.2.5)\n",
            "Collecting msgpack-numpy\n",
            "  Downloading https://files.pythonhosted.org/packages/57/8c/901d65deb827c0d9f7680ea808a0d63eab71fbfac5c6a868b6c9e92be4cb/msgpack_numpy-0.4.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy) (1.18.5)\n",
            "Installing collected packages: msgpack-numpy\n",
            "Successfully installed msgpack-numpy-0.4.7\n",
            "Collecting ffmpeg-python\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfwBKf0BOAHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CONFIG\n",
        "max_pose_detections = 1\n",
        "min_pose_score = 0.25\n",
        "min_part_score = 0.25\n",
        "SAVE_KEYPOINTS = True\n",
        "DEBUG = False\n",
        "SAVE_VIDEO = True"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiAT_qmmcYFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ffmpeg    \n",
        "# Fix rotation, code from https://stackoverflow.com/questions/53097092/frame-from-video-is-upside-down-after-extracting\n",
        "def check_rotation(path_video_file):\n",
        "    # this returns meta-data of the video file in form of a dictionary\n",
        "    meta_dict = ffmpeg.probe(path_video_file)\n",
        "\n",
        "    # from the dictionary, meta_dict['streams'][0]['tags']['rotate'] is the key\n",
        "    # we are looking for\n",
        "    rotateCode = None\n",
        "    if 'rotate' in meta_dict['streams'][0]['tags']:\n",
        "      if int(meta_dict['streams'][0]['tags']['rotate']) == 90:\n",
        "          rotateCode = cv2.ROTATE_90_CLOCKWISE\n",
        "      elif int(meta_dict['streams'][0]['tags']['rotate']) == 180:\n",
        "          rotateCode = cv2.ROTATE_180\n",
        "      elif int(meta_dict['streams'][0]['tags']['rotate']) == 270:\n",
        "          rotateCode = cv2.ROTATE_90_COUNTERCLOCKWISE\n",
        "\n",
        "    return rotateCode\n",
        "\n",
        "def correct_rotation(frame, rotateCode):  \n",
        "    return cv2.rotate(frame, rotateCode) "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTEnUCZPodtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PoseNet python sample program initialization\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import time\n",
        "import argparse\n",
        "import os\n",
        "import posenet\n",
        "import glob\n",
        "import msgpack\n",
        "import msgpack_numpy as m\n",
        "m.patch() #for handling numpy arrays w/ msgpack\n",
        "print('INIT:')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drivepath=\"/content/drive/My Drive/POSENET/\"\n",
        "# driveinfile = drivepath + 'input.mp4'\n",
        "video_path = glob.glob(drivepath + 'vids/*.mp4')\n",
        "save_video_path = drivepath + 'output_vids/'\n",
        "keypts_path = drivepath +'keypts/'\n",
        "labels_path = drivepath +'gesture_labels/'\n",
        "# VideoReaderWriter\n",
        "fourcc = cv2.VideoWriter_fourcc(*'H264')\n",
        "\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX \n",
        "org = (5, 30) \n",
        "fontScale = 1\n",
        "color = (0, 255, 0) \n",
        "thickness = 2\n",
        "   \n",
        "model = 101\n",
        "###scale_factor = 1.0\n",
        "scale_factor = 0.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6Q3yqU41lqI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "be279f22-a7c8-4c06-9373-0cbd0d16e5c3"
      },
      "source": [
        "# PoseNet python sample program run\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print('MODEL-INIT:')\n",
        "    ####model_cfg, model_outputs = posenet.load_model(args.model, sess)\n",
        "    model_cfg, model_outputs = posenet.load_model(model, sess)\n",
        "    output_stride = model_cfg['output_stride']\n",
        "    start = time.time()\n",
        "    print('START:')\n",
        "    for vid in video_path:\n",
        "      cap = cv2.VideoCapture(vid)\n",
        "\n",
        "      rotateCode = check_rotation(vid)\n",
        "\n",
        "      incnt = 0\n",
        "      keypoints_save = []\n",
        "      if SAVE_VIDEO:\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        driveoutfile = save_video_path + os.path.basename(vid)\n",
        "        video = cv2.VideoWriter(driveoutfile, fourcc, fps, (width, height))\n",
        "      while True:\n",
        "          incnt = incnt + 1\n",
        "          try: input_image, draw_image, output_scale = posenet.read_cap(\n",
        "                  cap, scale_factor=scale_factor, output_stride=output_stride,rotate_code=rotateCode)\n",
        "          except:break\n",
        "          heatmaps_result, offsets_result, displacement_fwd_result, displacement_bwd_result = sess.run(\n",
        "              model_outputs,\n",
        "              feed_dict={'image:0': input_image}\n",
        "          )\n",
        "          pose_scores, keypoint_scores, keypoint_coords = posenet.decode_multiple_poses(\n",
        "              heatmaps_result.squeeze(axis=0),\n",
        "              offsets_result.squeeze(axis=0),\n",
        "              displacement_fwd_result.squeeze(axis=0),\n",
        "              displacement_bwd_result.squeeze(axis=0),\n",
        "              output_stride=output_stride,\n",
        "              max_pose_detections=max_pose_detections,\n",
        "              min_pose_score=min_pose_score)\n",
        "\n",
        "          keypoint_coords *= output_scale\n",
        "\n",
        "          if SAVE_KEYPOINTS:\n",
        "            keypoints_save.append(keypoint_coords)\n",
        "          \n",
        "          if SAVE_VIDEO:\n",
        "            draw_image = posenet.draw_skel_and_kp(\n",
        "                    draw_image, pose_scores, keypoint_scores, keypoint_coords,\n",
        "                    min_pose_score=min_pose_score, min_part_score=min_part_score)\n",
        "            draw_image = cv2.putText(draw_image, str(incnt), org, font,  \n",
        "                   fontScale, color, thickness, cv2.LINE_AA)\n",
        "            video.write(draw_image)\n",
        "          if incnt % 100 == 0:        \n",
        "              print(\"cnt=\", incnt, \"fps=\", incnt / (time.time() - start) )\n",
        "\n",
        "          if DEBUG:\n",
        "              #debug\n",
        "              for pi in range(len(pose_scores)):\n",
        "                  if pose_scores[pi] == 0.:\n",
        "                      break\n",
        "                  print('Pose #%d, score = %f' % (pi, pose_scores[pi]))\n",
        "                  for ki, (s, c) in enumerate(zip(keypoint_scores[pi, :], keypoint_coords[pi, :, :])):\n",
        "                      print('Keypoint %s, score = %f, coord = %s' % (posenet.PART_NAMES[ki], s, c))\n",
        "      \n",
        "      cap.release()\n",
        "\n",
        "      if SAVE_KEYPOINTS:\n",
        "        with open(keypts_path + os.path.basename(vid)[0:-4] + \"_pts.msgpack\", 'wb') as f:\n",
        "          packed = msgpack.packb(keypoints_save)\n",
        "          f.write(packed)\n",
        "\n",
        "      if SAVE_VIDEO:\n",
        "         video.release()\n",
        "\n",
        "print('END:')\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INIT:\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "MODEL-INIT:\n",
            "START:\n",
            "cnt= 100 fps= 51.461118691485474\n",
            "cnt= 200 fps= 55.54660673435913\n",
            "END:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaWmO_WYcLsV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "313cae99-68ab-4d7c-efe4-5584cf54a545"
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "labels_path = drivepath +'gesture_labels/'\n",
        "for vid in video_path:\n",
        "    cap = cv2.VideoCapture(vid)\n",
        "    length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    labels=[0]*length\n",
        "    try:\n",
        "        df=pd.read_csv(labels_path + os.path.basename(vid)[0:-4] + \".csv\",names=[\"start\",\"end\",\"label\"])\n",
        "        for i,row in df.iterrows():\n",
        "            labels[row.start:row.end]=[row.label]*(row.end-row.start)\n",
        "        print(labels)\n",
        "    except:\n",
        "        print(os.path.basename(vid)+\"is missing the label file\")\n",
        "    with open(labels_path + os.path.basename(vid)[0:-4] + \".pickle\",\"wb\") as f:\n",
        "        pickle.dump(labels,f)\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Copy of VID_20200909_222404.mp4is missing the label file\n",
            "Copy of VID_20200909_223009.mp4is missing the label file\n",
            "Copy of VID_20200909_222531 (2).mp4is missing the label file\n",
            "Copy of VID_20200909_222722.mp4is missing the label file\n",
            "Copy of VID_20200909_223040.mp4is missing the label file\n",
            "Copy of VID_20200909_222603.mp4is missing the label file\n",
            "Copy of VID_20200909_222433.mp4is missing the label file\n",
            "Copy of VID_20200909_222221.mp4is missing the label file\n",
            "Copy of VID_20200909_222658.mp4is missing the label file\n",
            "Copy of VID_20200909_222151.mp4is missing the label file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu7uJWCzlXyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classes ---------------------------------------------------------------------\n",
        "class KerasBatchGenerator(object):\n",
        "\n",
        "    def __init__(self, file_list, batch_size,steps,jump=1):\n",
        "        self.file_list = file_list\n",
        "        self.batch_size = batch_size\n",
        "        self.steps=steps\n",
        "        self.current_idx = 0\n",
        "        self.current_file=0\n",
        "        self.data=None\n",
        "        self.label=None\n",
        "        self.limit=0\n",
        "        self.gestures=11\n",
        "        self.jump=jump\n",
        "        self.keypoint_number=17*2\n",
        "    def generate(self):\n",
        "        # Load June 1st data.\n",
        "        with open(keypts_path + self.file_list[self.current_file] + \"_pts.msgpack\", \"rb\") as data_file:\n",
        "            byte_data = data_file.read()\n",
        "        self.data = msgpack.unpackb(byte_data)\n",
        "        self.limit = len(self.data)\n",
        "        with open(labels_path + self.file_list[self.current_file] + \".pickle\", \"rb\") as label_file:\n",
        "            self.label=pickle.load(label_file)\n",
        "        while True:\n",
        "            \n",
        "            # Build empty matricies.\n",
        "            #y = np.zeros((self.batch_size, self.steps))\n",
        "            y = np.zeros((self.batch_size,self.steps, self.gestures))\n",
        "            keypoints = np.zeros((self.batch_size, self.steps, self.keypoint_number))\n",
        "\n",
        "            # Iterate through all files.\n",
        "            for i in range(self.batch_size):\n",
        "                if self.current_idx + self.steps + 1 >= self.limit:\n",
        "                    self.current_file += 1\n",
        "                    if(self.current_file >= len(self.file_list)):\n",
        "                        self.current_file =0\n",
        "                    with open(keypts_path + self.file_list[self.current_file] + \"_pts.msgpack\", \"rb\") as data_file:\n",
        "                        byte_data = data_file.read()\n",
        "                    self.data = msgpack.unpackb(byte_data)\n",
        "                    self.limit = len(self.data)\n",
        "                    self.current_idx = 0\n",
        "                    with open(labels_path + self.file_list[self.current_file] + \".pickle\", \"rb\") as label_file:\n",
        "                        self.label=pickle.load(label_file)\n",
        "                for k in range(self.steps):\n",
        "                    keypoints[i][k][:] = self.data[self.current_idx+k][0].flatten()\n",
        "                    y[i][k][:] = tf.keras.utils.to_categorical(self.label[self.current_idx+k],num_classes=self.gestures)\n",
        "                #### we need to decide on a syntax for the gesture time stamps and use it in label generation\n",
        "                self.current_idx += self.jump\n",
        "            x = keypoints\n",
        "            yield x, y\n",
        "            \n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agMQqHEisGcR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 670
        },
        "outputId": "28fdfa55-a1e6-4cb1-d554-edc0d3cee7f3"
      },
      "source": [
        "from tensorflow import keras\n",
        "import os\n",
        "import numpy as np\n",
        "# Global Variables ------------------------------------------------------------\n",
        "batch_size = 64\n",
        "steps = 24\n",
        "jump=8\n",
        "keypoint_number=17*2\n",
        "lstm_cels=keypoint_number\n",
        "gesture_number=11\n",
        "num_epochs=1\n",
        "training_keypoints=[\"input\" ]\n",
        "valid_keypoints=[\"input\" ]\n",
        "\n",
        "train_data_generator = KerasBatchGenerator(training_keypoints, batch_size, steps,jump)\n",
        "valid_data_generator = KerasBatchGenerator(valid_keypoints, batch_size, steps,jump)\n",
        "\n",
        "# Set Up GPU ------------------------------------------------------------------\n",
        "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\";\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\";\n",
        "\n",
        "# Build Models ----------------------------------------------------------------\n",
        "save_location='/content/drive/My Drive/POSENET/Experiments/ALL_LSTM_01'\n",
        "saveFiles = ([name for name in os.listdir(save_location)])\n",
        "initial_epoch = len(saveFiles)\n",
        "if(len(saveFiles) > 0):\n",
        "    if(len(saveFiles) > 9):\n",
        "        model = tf.keras.models.load_model(save_location+\"/model-\"+str(len(saveFiles))+\".hdf5\")\n",
        "    else:\n",
        "        model = tf.keras.models.load_model(save_location+\"/model-0\"+str(len(saveFiles))+\".hdf5\")\n",
        "    print(model.summary(90))\n",
        "else:\n",
        "    print(\"else\")\n",
        "    \n",
        "    # Build input shapes.\n",
        "    keypointInput = keras.layers.Input(shape=(steps, keypoint_number))\n",
        "\n",
        "    \n",
        "    # Build lstm layers.\n",
        "    masking_layer = keras.layers.Masking(mask_value=0)(keypointInput)\n",
        "    keypoint = keras.layers.LSTM(lstm_cels, return_sequences=True)(masking_layer)\n",
        "    # Build dense layer\n",
        "    dense_layer = keras.layers.Dense(gesture_number, activation='sigmoid')\n",
        "    gesture = keras.layers.TimeDistributed(dense_layer)(keypoint)\n",
        "    \n",
        "    \n",
        "    model = keras.Model(inputs=keypointInput, outputs=gesture)\n",
        "    print(model.summary(90))\n",
        "    \n",
        "# Weight and train model.\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',metrics=['accuracy','categorical_crossentropy'])\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath=save_location+'/concat-{epoch:02d}.hdf5', verbose=1)\n",
        "model.fit_generator(train_data_generator.generate(), 100, num_epochs,\n",
        "                    validation_data=valid_data_generator.generate(),validation_steps=10,\n",
        "                    callbacks=[checkpointer] ,\n",
        "                    shuffle=False,initial_epoch=initial_epoch)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "else\n",
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________\n",
            "Layer (type)                            Output Shape                        Param #       \n",
            "==========================================================================================\n",
            "input_5 (InputLayer)                    [(None, 24, 34)]                    0             \n",
            "__________________________________________________________________________________________\n",
            "masking_4 (Masking)                     (None, 24, 34)                      0             \n",
            "__________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                           (None, 24, 34)                      9384          \n",
            "__________________________________________________________________________________________\n",
            "time_distributed_4 (TimeDistributed)    (None, 24, 11)                      385           \n",
            "==========================================================================================\n",
            "Total params: 9,769\n",
            "Trainable params: 9,769\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "  273/10000 [..............................] - ETA: 11:40 - loss: 1.7393 - acc: 0.7516 - categorical_crossentropy: 1.7393"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-496d77aa233a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                     shuffle=False,initial_epoch=initial_epoch)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1015\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyBPBmZu1qww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "\n",
        "# TODO: Actually set-up the model beyond skeleton\n",
        "\n",
        "\n",
        "#num_gestures = 11 #10 gestures + null\n",
        "#batch = 32\n",
        "#num_epochs = 3\n",
        "\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Input()\n",
        "# model.add(LSTM())\n",
        "# model.add(LSTM())\n",
        "# model.add(Dense(num_gestures, activation='softmax'))\n",
        "# model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# X_train, X_valid, Y_train, Y_valid = subject_wise_split(X,Y)\n",
        "\n",
        "# model.fit(X_train, Y_train, batch_size=batch, epochs=num_epochs,verbose=1)\n",
        "\n",
        "# score, acc = model.evaluate(X_valid,Y_valid)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}